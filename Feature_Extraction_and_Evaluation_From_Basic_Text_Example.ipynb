{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3851c1-0d24-438e-a2ea-a344a7b1e4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to nltk_packages...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.data.path.append('nltk_packages')\n",
    "nltk.download('punkt_tab', download_dir='nltk_packages')\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae78d32-94ee-4a8d-994e-9cdee8b05457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words representation:\n",
      "[[0 0 1 0 0 0 0 0 1 1 0 1 2]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 1 2]\n",
      " [1 1 0 1 0 1 1 0 0 0 1 0 0]]\n",
      "Feature Names: ['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'great' 'log' 'mat' 'on' 'pets'\n",
      " 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\"\n",
    "]\n",
    "\n",
    "# Create the BoW model\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array and display\n",
    "bow_array = X_bow.toarray()\n",
    "print(\"Bag of Words representation:\")\n",
    "print(bow_array)\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae3232d-5e06-49d7-9b90-fb45b5a79579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF representation:\n",
      "[[0.         0.         0.42755362 0.         0.         0.\n",
      "  0.         0.         0.42755362 0.32516555 0.         0.32516555\n",
      "  0.6503311 ]\n",
      " [0.         0.         0.         0.         0.42755362 0.\n",
      "  0.         0.42755362 0.         0.32516555 0.         0.32516555\n",
      "  0.6503311 ]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
      "  0.40824829 0.         0.         0.         0.40824829 0.\n",
      "  0.        ]]\n",
      "Feature Names: ['and' 'are' 'cat' 'cats' 'dog' 'dogs' 'great' 'log' 'mat' 'on' 'pets'\n",
      " 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to array and display\n",
    "tfidf_array = X_tfidf.toarray()\n",
    "print(\"\\nTF-IDF representation:\")\n",
    "print(tfidf_array)\n",
    "print(\"Feature Names:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95097bd-fc1f-4a1a-8550-e2fcdbb8864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec representation for 'cat': [-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
      " -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
      " -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767582  0.00423474\n",
      "  0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
      "  0.00794257  0.00865741 -0.00989575 -0.0067557   0.00133757  0.0064403\n",
      "  0.00737382  0.00551698  0.00766163 -0.00512557  0.00658441 -0.00410837\n",
      " -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
      "  0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033661\n",
      "  0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
      " -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
      "  0.0050456   0.00671352  0.00451564  0.00866716  0.00747497 -0.00108189\n",
      "  0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
      " -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194235  0.00608616\n",
      "  0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
      "  0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
      "  0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
      "  0.00864797 -0.00313511 -0.00633882  0.00987008]\n",
      "Most similar words to 'cat': [('pets', 0.2529027760028839), ('.', 0.13725456595420837), ('great', 0.044109076261520386)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize documents into words\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model_w2v = gensim.models.Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Get vector for a specific word\n",
    "word_vector = model_w2v.wv['cat']\n",
    "print(\"\\nWord2Vec representation for 'cat':\", word_vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model_w2v.wv.most_similar('cat', topn=3)\n",
    "print(\"Most similar words to 'cat':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5fbbb3-a040-41ca-b735-3c9bcdcca77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'cat' and 'dogs': -0.025461027\n"
     ]
    }
   ],
   "source": [
    "similarity = model_w2v.wv.similarity('cat', 'dogs')  # Replace 'word1' and 'word2' with actual words\n",
    "print(\"Cosine similarity between 'cat' and 'dogs':\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df77a6f-abfa-48c9-b421-9638977dbde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GloVe representation for 'cat': [ 0.23088    0.28283    0.6318    -0.59411   -0.58599    0.63255\n",
      "  0.24402   -0.14108    0.060815  -0.7898    -0.29102    0.14287\n",
      "  0.72274    0.20428    0.1407     0.98757    0.52533    0.097456\n",
      "  0.8822     0.51221    0.40204    0.21169   -0.013109  -0.71616\n",
      "  0.55387    1.1452    -0.88044   -0.50216   -0.22814    0.023885\n",
      "  0.1072     0.083739   0.55015    0.58479    0.75816    0.45706\n",
      " -0.28001    0.25225    0.68965   -0.60972    0.19578    0.044209\n",
      " -0.31136   -0.68826   -0.22721    0.46185   -0.77162    0.10208\n",
      "  0.55636    0.067417  -0.57207    0.23735    0.4717     0.82765\n",
      " -0.29263   -1.3422    -0.099277   0.28139    0.41604    0.10583\n",
      "  0.62203    0.89496   -0.23446    0.51349    0.99379    1.1846\n",
      " -0.16364    0.20653    0.73854    0.24059   -0.96473    0.13481\n",
      " -0.0072484  0.33016   -0.12365    0.27191   -0.40951    0.021909\n",
      " -0.6069     0.40755    0.19566   -0.41802    0.18636   -0.032652\n",
      " -0.78571   -0.13847    0.044007  -0.084423   0.04911    0.24104\n",
      "  0.45273   -0.18682    0.46182    0.089068  -0.18185   -0.01523\n",
      " -0.7368    -0.14532    0.15104   -0.71493  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load pre-trained GloVe vectors (assuming 'glove.6B.100d.txt' is downloaded)\n",
    "glove_vectors = {}\n",
    "with open(\"datasets/nlp_pre_trained_word_embeddings/glove.6b/glove.6B.100d.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_vectors[word] = vector\n",
    "\n",
    "# Get vector for a specific word\n",
    "glove_vector_cat = glove_vectors.get('cat')\n",
    "print(\"\\nGloVe representation for 'cat':\", glove_vector_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c988dd-d75d-4aff-86ff-dbf803805081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FastText representation for 'dog': [-1.1338070e-03  2.0002944e-03 -2.5797717e-03  1.4003406e-03\n",
      " -2.4381576e-03 -3.4200454e-03  5.9867237e-04 -7.3052529e-04\n",
      "  1.1329837e-03  1.7838406e-04  3.0663244e-03  7.0723880e-04\n",
      "  1.7959451e-03  4.8864083e-03  4.6188425e-04 -2.9081232e-03\n",
      " -4.3916734e-04  7.4553618e-04 -7.3311763e-04 -2.0251889e-03\n",
      "  1.4080647e-03 -8.4824621e-04 -1.9739866e-03 -2.5472145e-03\n",
      " -4.6196635e-04  1.7570242e-03  1.7598222e-03  3.7484278e-03\n",
      "  2.1523852e-03 -1.4960765e-03 -5.4163342e-05 -1.9794821e-03\n",
      "  1.2051348e-05 -1.0916854e-03 -2.7709417e-03  2.5547245e-03\n",
      "  3.1367261e-03  1.1269423e-03 -4.6792114e-03 -6.3573674e-04\n",
      " -2.6462893e-03  1.9067300e-03  2.7983184e-03  1.9837550e-03\n",
      " -7.1428862e-04 -3.0612983e-03  1.8265255e-03  3.6188185e-03\n",
      " -1.3605955e-03 -6.1967788e-04  3.3724245e-03  8.3654711e-04\n",
      "  4.9976283e-03  2.9725847e-03 -2.8033410e-03  2.1066701e-03\n",
      " -1.7428529e-03 -1.3199404e-03  2.0273766e-03  1.1573492e-03\n",
      "  3.0376336e-03  4.5514925e-04  1.8565302e-03  4.1150110e-04\n",
      "  1.4781853e-03 -1.1311223e-03 -3.6711339e-05  2.5100147e-04\n",
      " -2.2835403e-03 -1.1358970e-03  3.9731152e-04  1.2775827e-03\n",
      "  2.3018846e-03  1.1558058e-03  7.8432314e-04  1.9514571e-03\n",
      "  2.2589296e-05  5.3490801e-03  2.3047703e-03 -2.6015311e-03\n",
      "  1.0247069e-03  1.6694928e-04  7.4706442e-04 -4.5020538e-03\n",
      "  1.6083036e-04  2.7132765e-04 -5.0358090e-04 -2.2699859e-03\n",
      "  1.0675658e-03  1.7242244e-03 -2.0347649e-04 -4.1555944e-03\n",
      " -2.3949356e-03  2.9203375e-03  5.7554138e-05  7.7698275e-04\n",
      "  3.6955460e-03 -1.0302622e-03  1.6286309e-03 -3.2823135e-03]\n",
      "Most similar words to 'dog': [('log', 0.38788971304893494), ('dogs', 0.3397655487060547), ('pets', 0.1264919489622116)]\n"
     ]
    }
   ],
   "source": [
    "# Train FastText model\n",
    "model_ft = FastText(sentences=tokenized_docs, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Get vector for a specific word\n",
    "fasttext_vector = model_ft.wv['dog']\n",
    "print(\"\\nFastText representation for 'dog':\", fasttext_vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_fasttext_words = model_ft.wv.most_similar('dog', topn=3)\n",
    "print(\"Most similar words to 'dog':\", similar_fasttext_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97884691-43c0-44d1-9204-16e27eb458b9",
   "metadata": {},
   "source": [
    "## Detailed Explanation of Techniques:\n",
    " - **Bag of Words:** This method creates a sparse matrix where each row represents a document and each column represents a unique word from the corpus. The values are counts of how often each word appears in each document.\n",
    "\n",
    "  - **TF-IDF:** This method builds on BoW by calculating how important a word is to a document relative to its frequency across all documents. It helps reduce the weight of common words that may not be informative.\n",
    "\n",
    "  - **Word2Vec:** This technique uses neural networks to learn embeddings that reflect semantic relationships based on context. It can be trained using two architectures: Continuous Bag of Words (CBOW) or Skip-Gram.\n",
    "GloVe: Unlike Word2Vec, which uses local context windows, GloVe captures global statistical information by factorizing a matrix of word co-occurrences.\n",
    "  - **FastText:** This method improves upon Word2Vec by considering subword information (character n-grams), which allows it to generate embeddings for words not seen during training.\n",
    "\n",
    "These techniques collectively enhance our ability to analyze and understand text data semantically and contextually, enabling more effective natural language processing applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
